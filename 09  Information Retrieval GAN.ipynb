{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Generative Advarsarial Network\n",
    "\n",
    "Here is presented the implementation of the following parper https://arxiv.org/abs/1705.10513 with help of their source code https://github.com/geek-ai/irgan\n",
    "\n",
    "Implementation is done in tensorflow using layers API\n",
    "\n",
    "Created by: Tiago Almeida 13/02/2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### imports\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import utils as ut # auxiliar file to help in data visualization\n",
    "from os.path import join\n",
    "\n",
    "### not mine, authors\n",
    "from eval_irgan.precision import precision_at_k\n",
    "from eval_irgan.ndcg import ndcg_at_k\n",
    "\n",
    "import random\n",
    "\n",
    "#tensorflow version when notebook was created - 1.4.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRGAN parameters\n",
    "\n",
    "From the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector size of features of the query-document pair\n",
    "FEATURE_SIZE = 46\n",
    "HIDDEN_SIZE = 46\n",
    "BATCH_SIZE = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "D_LEARNING_RATE = 0.001\n",
    "G_LEARNING_RATE = 0.001\n",
    "TEMPERATURE = 0.2\n",
    "LAMBDA = 0.5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "work_directory = join(\"data\",\"MQ2008-semi\")\n",
    "\n",
    "DIS_TRAIN_FILE = join(work_directory,\"run-train-gan.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset MQ2008-semi\n",
    "\n",
    "I'm using the same dataset as the authors https://drive.google.com/drive/folders/0B-dulzPp3MmCM01kYlhhNGQ0djA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download train.txt\n",
      "Download test.txt : 105MB\n",
      "Download Large_norm.txt : 524MB\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "train_data = join(work_directory,\"train.txt\")\n",
    "test_data = join(work_directory,\"test.txt\")\n",
    "large_norm = join(work_directory,\"Large_norm.txt\")\n",
    "\n",
    "##Download the data\n",
    "\n",
    "print(\"Download train.txt\")\n",
    "gdd.download_file_from_google_drive(file_id='0B-dulzPp3MmCUWNyTkt0azBhekE',\n",
    "                                    dest_path=train_data,\n",
    "                                    unzip=False)\n",
    "\n",
    "print(\"Download test.txt : 105MB\")\n",
    "gdd.download_file_from_google_drive(file_id='0B-dulzPp3MmCeUhJSWI5TWx0UTA',\n",
    "                                    dest_path=test_data,\n",
    "                                    unzip=False)\n",
    "\n",
    "print(\"Download Large_norm.txt : 524MB\")\n",
    "gdd.download_file_from_google_drive(file_id='0B-dulzPp3MmCT0hyN2hmbGlGUms',\n",
    "                                    dest_path=large_norm,\n",
    "                                    unzip=False)\n",
    "\n",
    "\n",
    "\n",
    "train_data = join(work_directory,\"train.txt\")\n",
    "test_data = join(work_directory,\"test.txt\")\n",
    "large_norm = join(work_directory,\"Large_norm.txt\")\n",
    "\n",
    "#get features of all query-document par from test and train set\n",
    "query_url_feature, query_url_index, query_index_url = ut.load_all_query_url_feature(large_norm, FEATURE_SIZE)\n",
    "\n",
    "#query-document pairs with positive relevance from train set\n",
    "query_pos_train = ut.get_query_pos(train_data)\n",
    "\n",
    "#query-document pairs with positive relevance from test set\n",
    "query_pos_test = ut.get_query_pos(test_data)\n",
    "\n",
    "query_all_features = {}\n",
    "for query in query_pos_train.keys():\n",
    "    query_all_features[query] = np.asarray([query_url_feature[query][url] for url in query_url_index[query]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, FEATURE_SIZE], name=\"input_generator\")\n",
    "\n",
    "def generator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1]) / TEMPERATURE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"pred_data_label\")\n",
    "\n",
    "def discriminator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_reward = tf.placeholder(tf.float32, shape=[None, FEATURE_SIZE], name='input_for_reward')\n",
    "sample_index = tf.placeholder(tf.int32, shape=[None], name='sample_index')\n",
    "important_sampling = tf.placeholder(tf.float32, shape=[None], name='important_sampling')\n",
    "\n",
    "gen_scores = generator(X,\"g\")\n",
    "dis_pred = discriminator(X,\"d\")\n",
    "#some discriminator but now using the reward placeholder\n",
    "dis_pred_reward = discriminator(X_for_reward,\"d\")\n",
    "\n",
    "generator_variables = [var for var in tf.trainable_variables() if 'g_' in var.name]\n",
    "discriminator_variables = [var for var in tf.trainable_variables() if 'd_' in var.name]\n",
    "        \n",
    "with tf.name_scope('discriminator_loss'):\n",
    "    d_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in discriminator_variables])\n",
    "    \n",
    "    d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_pred, labels=Y)) + WEIGHT_DECAY * d_weight_decay\n",
    "    \n",
    "    d_reward = (tf.sigmoid(dis_pred_reward) - 0.5) * 2\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"generator_loss\"):\n",
    "    gen_scores_prob = tf.nn.softmax(tf.reshape(gen_scores, [1, -1]))\n",
    "    \n",
    "    gan_prob = tf.gather(tf.reshape(gen_scores_prob, [-1]), sample_index)\n",
    "\n",
    "    g_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in generator_variables])\n",
    "    \n",
    "    g_loss = -tf.reduce_mean(tf.log(gan_prob) * d_reward * important_sampling) + WEIGHT_DECAY * g_weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (select) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# softmax for candidate\n",
    "prob_candidates = tf.nn.softmax(gen_scores - tf.reduce_max(gen_scores))\n",
    "    \n",
    "    \n",
    "def generate_for_d(sess, filename):\n",
    "    input_pos = []\n",
    "    input_neg = []\n",
    "    data = []\n",
    "    X_selected = []\n",
    "    Y_selected = []\n",
    "    \n",
    "    print('negative sampling for d using g ...')\n",
    "    for query in query_pos_train:\n",
    "        \n",
    "        \n",
    "        pos_list = query_pos_train[query] #positive documents\n",
    "        all_list = query_index_url[query] #all documents\n",
    "        candidate_list = all_list\n",
    "\n",
    "        #get features for possible selected documents\n",
    "        #candidate_list_feature = [query_url_feature[query][url] for url in candidate_list]\n",
    "        #candidate_list_feature = np.asarray(candidate_list_feature)\n",
    "        candidate_list_feature = query_all_features[query]\n",
    "        \n",
    "        \n",
    "        prob = sess.run(prob_candidates, feed_dict={X: candidate_list_feature})\n",
    "\n",
    "        #exp_rating = np.exp(candidate_list_score - np.max(candidate_list_score))\n",
    "        #prob = exp_rating / np.sum(exp_rating)\n",
    "        \n",
    "        # from all candidate choose same numbers of positive ones\n",
    "        neg_list = np.random.choice(candidate_list, size=[len(pos_list)], p=prob)\n",
    "        \n",
    "        \n",
    "        for i in range(len(pos_list)):\n",
    "            #data.append((query,pos_list[i],neg_list[i]))\n",
    "            X_selected.append(query_url_feature[query][pos_list[i]])\n",
    "            X_selected.append(query_url_feature[query][neg_list[i]])\n",
    "            Y_selected.append(1.0)\n",
    "            Y_selected.append(0.0)\n",
    "            #input_pos.append(query_url_feature[query][pos_list[i]])\n",
    "            #input_neg.append(query_url_feature[query][neg_list[i]])\n",
    "    \n",
    "    X_selected = np.asarray(X_selected)\n",
    "    Y_selected = np.asarray(Y_selected)\n",
    "    \n",
    "    #random_index = np.random.permutation(len(X_selected))\n",
    "    \n",
    "    return X_selected,Y_selected\n",
    "    \n",
    "    #return input_pos, input_neg\n",
    "    #save in disk\n",
    "    np.random.shuffle(data)\n",
    "    #print(\"num elements\",len(data))\n",
    "    with open(filename, 'w') as fout:\n",
    "        for (q, pos, neg) in data:\n",
    "            fout.write(','.join([str(f) for f in query_url_feature[q][pos]])\n",
    "                       + '\\t'\n",
    "                       + ','.join([str(f) for f in query_url_feature[q][neg]]) + '\\n')\n",
    "            fout.flush()\n",
    "    \n",
    "    return None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training D ...\n",
      "Epoch: 0\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.5260286331176758\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47591638565063477\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4618067741394043\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47934865951538086\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.22308411359786987\n",
      "Epoch generator: 0\n",
      "generator loss: -2.1836636 time 1.4049665927886963\n",
      "generator loss: -2.641133 time 1.3107802867889404\n",
      "generator loss: -3.010696 time 1.3171157836914062\n",
      "generator loss: -1.0139105 time 1.3177366256713867\n",
      "generator loss: -1.1820343 time 1.317194938659668\n",
      "Best: gen p@5  0.0019047619047619048 gen ndcg@5  0.0012495721667842075\n",
      "generator loss: -2.8786416 time 1.3221206665039062\n",
      "generator loss: -2.2844882 time 1.3161075115203857\n",
      "generator loss: -1.2561721 time 1.3173024654388428\n",
      "generator loss: -1.2483342 time 1.3178296089172363\n",
      "Best: gen p@5  0.0019047619047619048 gen ndcg@5  0.002259025835898059\n",
      "generator loss: -0.9654148 time 1.3146803379058838\n",
      "Best: gen p@5  0.005714285714285715 gen ndcg@5  0.007782996119816519\n",
      "Epoch generator: 10\n",
      "generator loss: -2.100329 time 1.3143954277038574\n",
      "generator loss: -1.7785602 time 1.3160758018493652\n",
      "Best: gen p@5  0.007619047619047619 gen ndcg@5  0.009779030406260072\n",
      "generator loss: -2.355961 time 1.316425085067749\n",
      "generator loss: -2.3171647 time 1.317014217376709\n",
      "generator loss: -3.5876806 time 1.2910032272338867\n",
      "generator loss: -1.646357 time 1.3138318061828613\n",
      "generator loss: -0.5111005 time 1.315117597579956\n",
      "Best: gen p@5  0.009523809523809525 gen ndcg@5  0.015529788653202918\n",
      "generator loss: -3.1166205 time 1.318997859954834\n",
      "generator loss: -0.26935792 time 1.316734790802002\n",
      "generator loss: -2.3937297 time 1.3122448921203613\n",
      "Epoch generator: 20\n",
      "generator loss: -0.55011785 time 1.3159558773040771\n",
      "generator loss: -2.5961974 time 1.314185619354248\n",
      "generator loss: -1.2833945 time 1.3160018920898438\n",
      "generator loss: -2.2068238 time 1.3292827606201172\n",
      "generator loss: -2.7622695 time 1.3159048557281494\n",
      "generator loss: -1.0653604 time 1.317920446395874\n",
      "generator loss: -1.476997 time 1.318925142288208\n",
      "generator loss: -1.8165255 time 1.3146698474884033\n",
      "generator loss: -0.7663389 time 1.3189787864685059\n",
      "generator loss: -1.1859639 time 1.2966179847717285\n",
      "Training D ...\n",
      "Epoch: 1\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47683072090148926\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47071242332458496\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47101783752441406\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47182559967041016\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.2233360517024994\n",
      "Epoch generator: 0\n",
      "generator loss: -0.41864014 time 1.309629201889038\n",
      "Best: gen p@5  0.009523809523809525 gen ndcg@5  0.01572878302752309\n",
      "generator loss: -1.662477 time 1.3102550506591797\n",
      "Best: gen p@5  0.011428571428571429 gen ndcg@5  0.018170487961346234\n",
      "generator loss: -1.245551 time 1.3042693138122559\n",
      "generator loss: -1.0267867 time 1.3071198463439941\n",
      "generator loss: -0.30274934 time 1.3102188110351562\n",
      "Best: gen p@5  0.013333333333333334 gen ndcg@5  0.019278505153555545\n",
      "generator loss: -0.66216505 time 1.3063108921051025\n",
      "generator loss: -2.2738519 time 1.3093125820159912\n",
      "generator loss: -2.3221564 time 1.3089158535003662\n",
      "generator loss: -1.6869764 time 1.309777021408081\n",
      "generator loss: -1.4102923 time 1.3047869205474854\n",
      "Epoch generator: 10\n",
      "generator loss: -1.8858145 time 1.3100485801696777\n",
      "generator loss: -1.8407995 time 1.312427282333374\n",
      "generator loss: -0.96384937 time 1.3092522621154785\n",
      "generator loss: -2.0935192 time 1.313377857208252\n",
      "generator loss: -2.9456694 time 1.3045837879180908\n",
      "generator loss: -2.487124 time 1.3078434467315674\n",
      "generator loss: -1.9509411 time 1.3127796649932861\n",
      "generator loss: -1.4123927 time 1.3138282299041748\n",
      "generator loss: -3.3721535 time 1.3023638725280762\n",
      "generator loss: -2.2713697 time 1.306473731994629\n",
      "Epoch generator: 20\n",
      "generator loss: -1.0211726 time 1.3122916221618652\n",
      "generator loss: -3.5549636 time 1.3077161312103271\n",
      "generator loss: -0.68908167 time 1.310518741607666\n",
      "generator loss: -3.548356 time 1.3057396411895752\n",
      "generator loss: -3.42708 time 1.3080573081970215\n",
      "generator loss: -2.1320565 time 1.3102588653564453\n",
      "generator loss: -1.3826845 time 1.3061678409576416\n",
      "generator loss: -2.6333284 time 1.3093194961547852\n",
      "generator loss: -2.561783 time 1.3115012645721436\n",
      "generator loss: -0.83076364 time 1.3046033382415771\n",
      "Training D ...\n",
      "Epoch: 2\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4718897342681885\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.46762514114379883\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4709641933441162\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47261857986450195\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.22330034335454305\n",
      "Epoch generator: 0\n",
      "generator loss: -1.5540459 time 1.3104376792907715\n",
      "generator loss: -0.7148074 time 1.3131883144378662\n",
      "generator loss: -2.7457933 time 1.307455062866211\n",
      "generator loss: -1.6608331 time 1.3091938495635986\n",
      "generator loss: -1.802406 time 1.307410717010498\n",
      "generator loss: -0.60967493 time 1.3105671405792236\n",
      "generator loss: -2.7506242 time 1.2585563659667969\n",
      "generator loss: -0.67312247 time 1.3087537288665771\n",
      "generator loss: -2.622451 time 1.3071129322052002\n",
      "generator loss: -1.1356401 time 1.3071322441101074\n",
      "Epoch generator: 10\n",
      "generator loss: -1.4965829 time 1.3148407936096191\n",
      "generator loss: -2.1794746 time 1.3115472793579102\n",
      "generator loss: -1.7289522 time 1.3102765083312988\n",
      "generator loss: -0.85998565 time 1.3126826286315918\n",
      "generator loss: -2.3082838 time 1.3088877201080322\n",
      "generator loss: -0.3105047 time 1.3105754852294922\n",
      "generator loss: -2.3637412 time 1.3112318515777588\n",
      "generator loss: -1.3461525 time 1.313164472579956\n",
      "generator loss: -1.8545895 time 1.3093044757843018\n",
      "generator loss: -1.5556695 time 1.3100099563598633\n",
      "Epoch generator: 20\n",
      "generator loss: -1.4932035 time 1.3161983489990234\n",
      "generator loss: -3.0730731 time 1.2964482307434082\n",
      "generator loss: -2.9920652 time 1.3145239353179932\n",
      "generator loss: -1.8657196 time 1.3047778606414795\n",
      "generator loss: -2.897597 time 1.3100571632385254\n",
      "generator loss: -1.8735994 time 1.3155570030212402\n",
      "generator loss: -0.09166698 time 1.3117163181304932\n",
      "generator loss: -0.8338917 time 1.3096823692321777\n",
      "generator loss: -0.6158014 time 1.3123693466186523\n",
      "generator loss: -3.1428192 time 1.2983386516571045\n",
      "Training D ...\n",
      "Epoch: 3\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4762887954711914\n",
      "4754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.name_scope(\"generator_train\"):\n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(G_LEARNING_RATE)\n",
    "    g_train_op = g_optimizer.minimize(g_loss, var_list=generator_variables)\n",
    "    \n",
    "with tf.name_scope(\"discriminator_train\"):\n",
    "    d_optimizer = tf.train.GradientDescentOptimizer(D_LEARNING_RATE)\n",
    "    d_train_op = d_optimizer.minimize(d_loss, var_list=discriminator_variables)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "## Start graph computations and algorithm\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "p_best_val = 0.0\n",
    "ndcg_best_val = 0.0\n",
    "\n",
    "generate_samples_time = 0\n",
    "generate_epoch_time = 0\n",
    "discriminator_record_time = []\n",
    "\n",
    "discriminator_train_time = 0\n",
    "\n",
    "prob_is_candidates = prob_candidates * (1.0 - LAMBDA)\n",
    "\n",
    "for epoch in range(30):\n",
    "    if epoch >= 0:\n",
    "        # G generate negative for D, then train D\n",
    "        print('Training D ...')\n",
    "        print(\"Epoch:\",epoch)\n",
    "        for d_epoch in range(100):\n",
    "\n",
    "            if d_epoch % 30 == 0:\n",
    "                generate_samples_time = time.time()\n",
    "                selected_samples_X, selected_samples_Y = generate_for_d(sess, DIS_TRAIN_FILE)\n",
    "                print(\"time used in generation\",time.time()-generate_samples_time)\n",
    "                train_size = len(selected_samples_X)\n",
    "                #train_size = ut.file_len(DIS_TRAIN_FILE)\n",
    "                print(train_size)\n",
    "            \n",
    "            discriminator_train_time = time.time()\n",
    "            index = 0\n",
    "            while True:\n",
    "                if index >= train_size:\n",
    "                    break\n",
    "                if index + BATCH_SIZE < train_size:\n",
    "                    #input_pos = selected_samples_pos[index:index+BATCH_SIZE]\n",
    "                    #input_neg = selected_samples_neg[index:index+BATCH_SIZE]\n",
    "                    pred_data = selected_samples_X[index:index+BATCH_SIZE]\n",
    "                    pred_data_label = selected_samples_Y[index:index+BATCH_SIZE]\n",
    "                    #input_pos,input_neg = ut.get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n",
    "                else:\n",
    "                    #input_pos = selected_samples_pos[index:index+train_size - index ] \n",
    "                    #input_neg = selected_samples_neg[index:index+train_size - index ]\n",
    "                    pred_data = selected_samples_X[index:train_size]\n",
    "                    pred_data_label = selected_samples_Y[index:train_size]\n",
    "                    #input_pos,input_neg = ut.get_batch_data(DIS_TRAIN_FILE, index, train_size - index + 1)\n",
    "                index += BATCH_SIZE\n",
    "                \"\"\"\n",
    "                #print(index)\n",
    "            \n",
    "                pred_data = []\n",
    "                pred_data.extend(input_pos)\n",
    "                pred_data.extend(input_neg)\n",
    "                pred_data = np.asarray(pred_data)\n",
    "                \n",
    "                #print(pred_data.shape)\n",
    "                \n",
    "                pred_data_label = [1.0] * len(input_pos)\n",
    "                pred_data_label.extend([0.0] * len(input_neg))\n",
    "                pred_data_label = np.asarray(pred_data_label)\n",
    "                \"\"\"\n",
    "                _ = sess.run(d_train_op,\n",
    "                             feed_dict={X: pred_data,\n",
    "                                        Y: pred_data_label})\n",
    "                \n",
    "            discriminator_record_time += [time.time()-discriminator_train_time]\n",
    "            \n",
    "    # Train G\n",
    "    print('Training G ...')\n",
    "    print(\"AVG discriminator train time\", sum(discriminator_record_time)/len(discriminator_record_time))\n",
    "    for g_epoch in range(30):\n",
    "        \n",
    "        if g_epoch%10==0:\n",
    "            print(\"Epoch generator:\",g_epoch)\n",
    "            \n",
    "        generate_epoch_time = time.time()\n",
    "        \n",
    "        for query in query_pos_train.keys():\n",
    "            pos_list = query_pos_train[query]\n",
    "            pos_set = set(pos_list)\n",
    "            all_list = query_index_url[query]\n",
    "\n",
    "            #all_list_feature = [query_url_feature[query][url] for url in all_list]\n",
    "            all_list_feature = query_all_features[query]\n",
    "            #all_list_score = sess.run(prob_candidates, {X: all_list_feature})\n",
    "\n",
    "            # softmax for all\n",
    "            #exp_rating = np.exp(all_list_score - np.max(all_list_score))\n",
    "            #prob = exp_rating / np.sum(exp_rating)\n",
    "            \n",
    "            prob_IS, prob = sess.run([prob_is_candidates,prob_candidates], {X: all_list_feature})\n",
    "            #prob_IS = prob * (1.0 - LAMBDA)\n",
    "\n",
    "            for i in range(len(all_list)):\n",
    "                if all_list[i] in pos_set:\n",
    "                    prob_IS[i] += (LAMBDA / (1.0 * len(pos_list)))\n",
    "\n",
    "            choose_index = np.random.choice(np.arange(len(all_list)), [5 * len(pos_list)], p=prob_IS)\n",
    "            choose_list = np.array(all_list)[choose_index]\n",
    "            choose_feature = [query_url_feature[query][url] for url in choose_list]\n",
    "            choose_IS = np.array(prob)[choose_index] / np.array(prob_IS)[choose_index]\n",
    "\n",
    "            choose_index = np.asarray(choose_index)\n",
    "            choose_feature = np.asarray(choose_feature)\n",
    "            choose_IS = np.asarray(choose_IS)\n",
    "\n",
    "            _,loss = sess.run([g_train_op,g_loss],\n",
    "                         feed_dict={X: all_list_feature,\n",
    "                                    sample_index: choose_index,\n",
    "                                    X_for_reward: choose_feature,\n",
    "                                    important_sampling: choose_IS})\n",
    "\n",
    "        print(\"generator loss:\",loss,\"time\",time.time()-generate_epoch_time)\n",
    "        p_5 = precision_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "        ndcg_5 = ndcg_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "        \n",
    "        if p_5 > p_best_val:\n",
    "            p_best_val = p_5\n",
    "            ndcg_best_val = ndcg_5\n",
    "            print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n",
    "        elif p_5 == p_best_val:\n",
    "            if ndcg_5 > ndcg_best_val:\n",
    "                ndcg_best_val = ndcg_5\n",
    "                print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
