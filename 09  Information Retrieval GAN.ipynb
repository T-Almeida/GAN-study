{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Generative Advarsarial Network\n",
    "\n",
    "Here is presented the implementation of the following parper https://arxiv.org/abs/1705.10513 with help of their source code https://github.com/geek-ai/irgan\n",
    "\n",
    "Implementation is done in tensorflow using layers API\n",
    "\n",
    "Created by: Tiago Almeida 13/02/2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tiago\\anaconda3\\envs\\tf-1.9-gpu\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "c:\\users\\tiago\\anaconda3\\envs\\tf-1.9-gpu\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### imports\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import utils as ut # auxiliar file to help in data visualization\n",
    "from os.path import join\n",
    "\n",
    "### not mine, authors\n",
    "from eval_irgan.precision import precision_at_k\n",
    "from eval_irgan.ndcg import ndcg_at_k\n",
    "\n",
    "import random\n",
    "\n",
    "#tensorflow version when notebook was created - 1.4.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRGAN parameters\n",
    "\n",
    "From the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector size of features of the query-document pair\n",
    "FEATURE_SIZE = 46\n",
    "HIDDEN_SIZE = 46\n",
    "BATCH_SIZE = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "D_LEARNING_RATE = 0.001\n",
    "G_LEARNING_RATE = 0.001\n",
    "TEMPERATURE = 0.2\n",
    "LAMBDA = 0.5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "work_directory = join(\"data\",\"MQ2008-semi\")\n",
    "\n",
    "DIS_TRAIN_FILE = join(work_directory,\"run-train-gan.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset MQ2008-semi\n",
    "\n",
    "I'm using the same dataset as the authors https://drive.google.com/drive/folders/0B-dulzPp3MmCM01kYlhhNGQ0djA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = join(work_directory,\"train.txt\")\n",
    "test_data = join(work_directory,\"test.txt\")\n",
    "large_norm = join(work_directory,\"Large_norm.txt\")\n",
    "\n",
    "#get features of all query-document par from test and train set\n",
    "query_url_feature, query_url_index, query_index_url = ut.load_all_query_url_feature(large_norm, FEATURE_SIZE)\n",
    "\n",
    "#query-document pairs with positive relevance from train set\n",
    "query_pos_train = ut.get_query_pos(train_data)\n",
    "\n",
    "#query-document pairs with positive relevance from test set\n",
    "query_pos_test = ut.get_query_pos(test_data)\n",
    "\n",
    "query_all_features = {}\n",
    "for query in query_pos_train.keys():\n",
    "    query_all_features[query] = np.asarray([query_url_feature[query][url] for url in query_url_index[query]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, FEATURE_SIZE], name=\"input_generator\")\n",
    "\n",
    "def generator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1]) / TEMPERATURE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"pred_data_label\")\n",
    "\n",
    "def discriminator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_reward = tf.placeholder(tf.float32, shape=[None, FEATURE_SIZE], name='input_for_reward')\n",
    "sample_index = tf.placeholder(tf.int32, shape=[None], name='sample_index')\n",
    "important_sampling = tf.placeholder(tf.float32, shape=[None], name='important_sampling')\n",
    "\n",
    "gen_scores = generator(X,\"g\")\n",
    "dis_pred = discriminator(X,\"d\")\n",
    "#some discriminator but now using the reward placeholder\n",
    "dis_pred_reward = discriminator(X_for_reward,\"d\")\n",
    "\n",
    "generator_variables = [var for var in tf.trainable_variables() if 'g_' in var.name]\n",
    "discriminator_variables = [var for var in tf.trainable_variables() if 'd_' in var.name]\n",
    "        \n",
    "with tf.name_scope('discriminator_loss'):\n",
    "    d_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in discriminator_variables])\n",
    "    \n",
    "    d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_pred, labels=Y)) + WEIGHT_DECAY * d_weight_decay\n",
    "    \n",
    "    d_reward = (tf.sigmoid(dis_pred_reward) - 0.5) * 2\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"generator_loss\"):\n",
    "    gen_scores_prob = tf.nn.softmax(tf.reshape(gen_scores, [1, -1]))\n",
    "    \n",
    "    gan_prob = tf.gather(tf.reshape(gen_scores_prob, [-1]), sample_index)\n",
    "\n",
    "    g_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in generator_variables])\n",
    "    \n",
    "    g_loss = -tf.reduce_mean(tf.log(gan_prob) * d_reward * important_sampling) + WEIGHT_DECAY * g_weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (select) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# softmax for candidate\n",
    "prob_candidates = tf.nn.softmax(gen_scores - tf.reduce_max(gen_scores))\n",
    "    \n",
    "    \n",
    "def generate_for_d(sess, filename):\n",
    "    input_pos = []\n",
    "    input_neg = []\n",
    "    data = []\n",
    "    X_selected = []\n",
    "    Y_selected = []\n",
    "    \n",
    "    print('negative sampling for d using g ...')\n",
    "    for query in query_pos_train:\n",
    "        \n",
    "        \n",
    "        pos_list = query_pos_train[query] #positive documents\n",
    "        all_list = query_index_url[query] #all documents\n",
    "        candidate_list = all_list\n",
    "\n",
    "        #get features for possible selected documents\n",
    "        #candidate_list_feature = [query_url_feature[query][url] for url in candidate_list]\n",
    "        #candidate_list_feature = np.asarray(candidate_list_feature)\n",
    "        candidate_list_feature = query_all_features[query]\n",
    "        \n",
    "        \n",
    "        prob = sess.run(prob_candidates, feed_dict={X: candidate_list_feature})\n",
    "\n",
    "        #exp_rating = np.exp(candidate_list_score - np.max(candidate_list_score))\n",
    "        #prob = exp_rating / np.sum(exp_rating)\n",
    "        \n",
    "        # from all candidate choose same numbers of positive ones\n",
    "        neg_list = np.random.choice(candidate_list, size=[len(pos_list)], p=prob)\n",
    "        \n",
    "        \n",
    "        for i in range(len(pos_list)):\n",
    "            #data.append((query,pos_list[i],neg_list[i]))\n",
    "            X_selected.append(query_url_feature[query][pos_list[i]])\n",
    "            X_selected.append(query_url_feature[query][neg_list[i]])\n",
    "            Y_selected.append(1.0)\n",
    "            Y_selected.append(0.0)\n",
    "            #input_pos.append(query_url_feature[query][pos_list[i]])\n",
    "            #input_neg.append(query_url_feature[query][neg_list[i]])\n",
    "    \n",
    "    X_selected = np.asarray(X_selected)\n",
    "    Y_selected = np.asarray(Y_selected)\n",
    "    \n",
    "    #random_index = np.random.permutation(len(X_selected))\n",
    "    \n",
    "    return X_selected,Y_selected\n",
    "    \n",
    "    #return input_pos, input_neg\n",
    "    #save in disk\n",
    "    np.random.shuffle(data)\n",
    "    #print(\"num elements\",len(data))\n",
    "    with open(filename, 'w') as fout:\n",
    "        for (q, pos, neg) in data:\n",
    "            fout.write(','.join([str(f) for f in query_url_feature[q][pos]])\n",
    "                       + '\\t'\n",
    "                       + ','.join([str(f) for f in query_url_feature[q][neg]]) + '\\n')\n",
    "            fout.flush()\n",
    "    \n",
    "    return None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tiago\\anaconda3\\envs\\tf-1.9-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training D ...\n",
      "Epoch: 0\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.9224107265472412\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4856562614440918\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.48563122749328613\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47914600372314453\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.5562081336975098\n",
      "Epoch generator: 0\n",
      "generator loss: -1.5330174 time 1.673095464706421\n",
      "Best: gen p@5  0.005714285714285715 gen ndcg@5  0.006236272930982197\n",
      "generator loss: -1.5871792 time 1.4725816249847412\n",
      "Best: gen p@5  0.013333333333333332 gen ndcg@5  0.014145033858250692\n",
      "generator loss: -1.4211513 time 1.4347965717315674\n",
      "generator loss: -1.4613438 time 1.4569571018218994\n",
      "Best: gen p@5  0.01714285714285714 gen ndcg@5  0.023097152978908634\n",
      "generator loss: -1.4230883 time 1.4412901401519775\n",
      "Best: gen p@5  0.01714285714285714 gen ndcg@5  0.024654762175704385\n",
      "generator loss: -2.7185109 time 1.5571672916412354\n",
      "generator loss: -2.2914019 time 1.4413208961486816\n",
      "generator loss: -1.3689691 time 1.4504919052124023\n",
      "generator loss: -1.5752854 time 1.434816598892212\n",
      "generator loss: -2.2853694 time 1.4348244667053223\n",
      "Epoch generator: 10\n",
      "generator loss: -1.2858865 time 1.4504425525665283\n",
      "generator loss: -1.4118347 time 1.441361665725708\n",
      "Best: gen p@5  0.019047619047619046 gen ndcg@5  0.024488280120267745\n",
      "generator loss: -1.7042019 time 1.4413495063781738\n",
      "Best: gen p@5  0.02095238095238095 gen ndcg@5  0.027118694965666726\n",
      "generator loss: -0.9793111 time 1.4413106441497803\n",
      "generator loss: -0.70513374 time 1.4413347244262695\n",
      "generator loss: -0.28226995 time 1.441293478012085\n",
      "generator loss: -0.8659418 time 1.4413318634033203\n",
      "generator loss: -1.0556873 time 1.441300630569458\n",
      "generator loss: -0.6532578 time 1.4413831233978271\n",
      "generator loss: -0.67320526 time 1.4413349628448486\n",
      "Epoch generator: 20\n",
      "generator loss: -1.036164 time 1.4347681999206543\n",
      "generator loss: -0.85942596 time 1.4348080158233643\n",
      "generator loss: -1.0009624 time 1.4504685401916504\n",
      "generator loss: -0.9216061 time 1.419170618057251\n",
      "generator loss: -1.5769235 time 1.4257183074951172\n",
      "generator loss: -1.1957233 time 1.45039701461792\n",
      "generator loss: -0.7783586 time 1.4504268169403076\n",
      "generator loss: -0.8687594 time 1.4348130226135254\n",
      "generator loss: -0.7208212 time 1.4348127841949463\n",
      "generator loss: -1.384377 time 1.4413256645202637\n",
      "Training D ...\n",
      "Epoch: 1\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4700145721435547\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4478929042816162\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4635138511657715\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4478797912597656\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.5486575055122376\n",
      "Epoch generator: 0\n",
      "generator loss: -0.7850416 time 1.4413268566131592\n",
      "generator loss: -1.3729903 time 1.4348058700561523\n",
      "generator loss: -0.727183 time 1.4347996711730957\n",
      "generator loss: -0.69035774 time 1.4504306316375732\n",
      "generator loss: -1.0601909 time 1.4413537979125977\n",
      "generator loss: -1.517998 time 1.425675630569458\n",
      "generator loss: -0.6768702 time 1.434800624847412\n",
      "generator loss: -0.7514935 time 1.434800624847412\n",
      "generator loss: -1.1631124 time 1.4256739616394043\n",
      "generator loss: -0.77534634 time 1.4347941875457764\n",
      "Epoch generator: 10\n",
      "generator loss: -1.0751584 time 1.4347753524780273\n",
      "generator loss: -0.8567512 time 1.4347968101501465\n",
      "generator loss: -0.76318896 time 1.450444221496582\n",
      "generator loss: -0.860494 time 1.4413392543792725\n",
      "generator loss: -0.8867432 time 1.4192230701446533\n",
      "generator loss: -0.64320344 time 1.4504063129425049\n",
      "generator loss: -0.78254217 time 1.434849500656128\n",
      "generator loss: -1.067348 time 1.4257006645202637\n",
      "generator loss: -0.72090393 time 1.4413280487060547\n",
      "generator loss: -0.33178198 time 1.425685167312622\n",
      "Epoch generator: 20\n",
      "generator loss: -0.8405273 time 1.441310167312622\n",
      "generator loss: -0.59888315 time 1.4412891864776611\n",
      "generator loss: -0.8354979 time 1.4192099571228027\n",
      "generator loss: -0.8457247 time 1.4413461685180664\n",
      "generator loss: -1.0702668 time 1.4413201808929443\n",
      "generator loss: -0.7002496 time 1.4347712993621826\n",
      "generator loss: -0.7339416 time 1.4504213333129883\n",
      "generator loss: -0.90662867 time 1.434798240661621\n",
      "generator loss: -0.94015706 time 1.4347801208496094\n",
      "generator loss: -0.6143646 time 1.4348080158233643\n",
      "Training D ...\n",
      "Epoch: 2\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4543924331665039\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4635157585144043\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4700167179107666\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4544050693511963\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.5471524588267008\n",
      "Epoch generator: 0\n",
      "generator loss: -0.60127413 time 1.4413602352142334\n",
      "generator loss: -0.79412645 time 1.425718069076538\n",
      "generator loss: -0.56948525 time 1.4348409175872803\n",
      "generator loss: -0.92742723 time 1.450453519821167\n",
      "generator loss: -0.9093299 time 1.45041823387146\n",
      "generator loss: -0.48459882 time 1.4348158836364746\n",
      "generator loss: -0.41212785 time 1.4347858428955078\n",
      "generator loss: -0.5835155 time 1.4348104000091553\n",
      "generator loss: -0.7328041 time 1.4347970485687256\n",
      "generator loss: -0.92967445 time 1.4504053592681885\n",
      "Epoch generator: 10\n",
      "generator loss: -0.7701688 time 1.434767723083496\n",
      "generator loss: -0.84864736 time 1.4348320960998535\n",
      "generator loss: -0.5512274 time 1.425694227218628\n",
      "generator loss: -0.7922546 time 1.4348390102386475\n",
      "generator loss: -0.670081 time 1.4348008632659912\n",
      "generator loss: -0.8371659 time 1.4347867965698242\n",
      "generator loss: -0.4649502 time 1.4348163604736328\n",
      "generator loss: -0.51574755 time 1.434837818145752\n",
      "generator loss: -0.91334116 time 1.44130539894104\n",
      "generator loss: -1.0346875 time 1.4413471221923828\n",
      "Epoch generator: 20\n",
      "generator loss: -0.7173836 time 1.450425624847412\n",
      "generator loss: -0.8885292 time 1.4348034858703613\n",
      "generator loss: -0.21306913 time 1.4256749153137207\n",
      "generator loss: -0.6641809 time 1.4347848892211914\n",
      "generator loss: -0.49720705 time 1.4347953796386719\n",
      "generator loss: -0.67687476 time 1.450439453125\n",
      "generator loss: -0.5563968 time 1.4348077774047852\n",
      "generator loss: -0.51661706 time 1.4412879943847656\n",
      "generator loss: -0.50115365 time 1.4569487571716309\n",
      "generator loss: -0.7713604 time 1.4569597244262695\n",
      "Training D ...\n",
      "Epoch: 3\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4635138511657715\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.45438241958618164\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4700310230255127\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4635195732116699\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.5446617335081101\n",
      "Epoch generator: 0\n",
      "generator loss: -0.6131073 time 1.4347896575927734\n",
      "generator loss: -0.24085423 time 1.4347739219665527\n",
      "generator loss: -0.5559883 time 1.434790849685669\n",
      "generator loss: -0.25282624 time 1.5259904861450195\n",
      "generator loss: -0.54535127 time 1.4348011016845703\n",
      "generator loss: -0.15554908 time 1.4348294734954834\n",
      "generator loss: -0.030119345 time 1.434783935546875\n",
      "generator loss: -0.9311029 time 1.4348554611206055\n",
      "generator loss: -0.54714614 time 1.4256656169891357\n",
      "generator loss: -0.49078423 time 1.4347882270812988\n",
      "Epoch generator: 10\n",
      "generator loss: -0.37621564 time 1.4347867965698242\n",
      "generator loss: -0.38896632 time 1.4348058700561523\n",
      "generator loss: -0.88703144 time 1.4413340091705322\n",
      "generator loss: -0.883131 time 1.4347901344299316\n",
      "generator loss: -0.1919899 time 1.4504592418670654\n",
      "generator loss: -0.5771492 time 1.4347822666168213\n",
      "generator loss: -0.2937172 time 1.4503982067108154\n",
      "generator loss: -0.62921405 time 1.4348349571228027\n",
      "generator loss: -0.4325656 time 1.434765338897705\n",
      "generator loss: -0.81821865 time 1.4347918033599854\n",
      "Epoch generator: 20\n",
      "generator loss: -0.2592856 time 1.4348344802856445\n",
      "generator loss: -0.19979468 time 1.4348421096801758\n",
      "generator loss: -0.19731522 time 1.4347748756408691\n",
      "generator loss: -0.24027878 time 1.4413392543792725\n",
      "generator loss: -0.17812341 time 1.419189214706421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loss: -0.25624928 time 1.4348118305206299\n",
      "generator loss: -0.28041896 time 1.4347736835479736\n",
      "generator loss: -0.40457702 time 1.4256701469421387\n",
      "generator loss: -0.39374352 time 1.441279411315918\n",
      "generator loss: -0.32383454 time 1.450439214706421\n",
      "Training D ...\n",
      "Epoch: 4\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.5859074592590332\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.45439982414245605\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4700338840484619\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.46352314949035645\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.543862446308136\n",
      "Epoch generator: 0\n",
      "generator loss: -0.4804961 time 1.4413275718688965\n",
      "generator loss: -0.3217991 time 1.4347848892211914\n",
      "generator loss: -0.60737604 time 1.4348132610321045\n",
      "generator loss: -0.090603895 time 1.4348382949829102\n",
      "generator loss: -0.24442518 time 1.4256823062896729\n",
      "generator loss: -0.22516048 time 1.4256727695465088\n",
      "generator loss: -0.33165085 time 1.4347753524780273\n",
      "generator loss: -0.17226994 time 1.4348058700561523\n",
      "generator loss: -0.736336 time 1.4347927570343018\n",
      "generator loss: -0.47917858 time 1.4348106384277344\n",
      "Epoch generator: 10\n",
      "generator loss: -0.31514883 time 1.441312551498413\n",
      "generator loss: -0.28274608 time 1.4256644248962402\n",
      "generator loss: -0.13909355 time 1.434831142425537\n",
      "generator loss: -0.42024812 time 1.425682783126831\n",
      "generator loss: -0.2180851 time 1.4413037300109863\n",
      "generator loss: -0.48767054 time 1.4348115921020508\n",
      "generator loss: 0.0014927909 time 1.434800386428833\n",
      "generator loss: -0.565514 time 1.4348516464233398\n",
      "generator loss: -0.26954767 time 1.4347975254058838\n",
      "generator loss: -0.19644302 time 1.4256610870361328\n",
      "Epoch generator: 20\n",
      "generator loss: -0.39656445 time 1.434797763824463\n",
      "generator loss: -0.37211323 time 1.4348139762878418\n",
      "generator loss: -0.17589769 time 1.4348134994506836\n",
      "generator loss: -0.121814474 time 1.441303014755249\n",
      "generator loss: -0.194579 time 1.4191417694091797\n",
      "generator loss: -0.45849007 time 1.43479585647583\n",
      "generator loss: -0.57171595 time 1.419191598892212\n",
      "generator loss: -0.14301819 time 1.4257087707519531\n",
      "generator loss: -0.14938155 time 1.434814214706421\n",
      "generator loss: 0.079975575 time 1.419156551361084\n",
      "Training D ...\n",
      "Epoch: 5\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.46352553367614746\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.470015287399292\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47001218795776367\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.44788479804992676\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.5433033156394959\n",
      "Epoch generator: 0\n",
      "generator loss: -0.25144 time 1.4256746768951416\n",
      "generator loss: -0.25236237 time 1.4348227977752686\n",
      "generator loss: -0.19829613 time 1.7200334072113037\n",
      "generator loss: -0.27303025 time 1.43479585647583\n",
      "generator loss: -0.20988704 time 1.4412815570831299\n",
      "generator loss: -0.33296064 time 1.434828281402588\n",
      "generator loss: -0.39107233 time 1.4348087310791016\n",
      "generator loss: -0.3466578 time 1.4191741943359375\n",
      "generator loss: -0.31007418 time 1.4882218837738037\n",
      "generator loss: -0.14314812 time 1.4348063468933105\n",
      "Epoch generator: 10\n",
      "generator loss: -0.9173642 time 1.41916823387146\n",
      "generator loss: -0.3258462 time 1.4347920417785645\n",
      "generator loss: -0.14522 time 1.450432538986206\n",
      "generator loss: -0.13552988 time 1.450408697128296\n",
      "generator loss: -0.5213615 time 1.4191741943359375\n",
      "generator loss: -0.42335612 time 1.4412732124328613\n",
      "generator loss: -0.06370928 time 1.450486660003662\n",
      "generator loss: -0.4105395 time 1.4347846508026123\n",
      "generator loss: -0.20538865 time 1.4348092079162598\n",
      "generator loss: -0.06420975 time 1.434805154800415\n",
      "Epoch generator: 20\n",
      "generator loss: -0.25307906 time 1.4191608428955078\n",
      "generator loss: -0.26127535 time 1.4348070621490479\n",
      "generator loss: 0.001950793 time 1.4412884712219238\n",
      "generator loss: -0.32998544 time 1.4347953796386719\n",
      "generator loss: -0.405089 time 1.4256818294525146\n",
      "generator loss: -0.2853949 time 1.4412925243377686\n",
      "generator loss: -0.3139103 time 1.4348089694976807\n",
      "generator loss: -0.31280947 time 1.4348125457763672\n",
      "generator loss: -0.18269527 time 1.4504380226135254\n",
      "generator loss: -0.15018955 time 1.4413623809814453\n",
      "Training D ...\n",
      "Epoch: 6\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.45440244674682617\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47002387046813965\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4543883800506592\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4543931484222412\n",
      "4754\n",
      "Training G ...\n",
      "AVG discriminator train time 0.543248017174857\n",
      "Epoch generator: 0\n",
      "generator loss: -0.3354284 time 1.4413189888000488\n",
      "generator loss: -0.26837593 time 1.4504244327545166\n",
      "generator loss: -0.27952692 time 1.466066598892212\n",
      "generator loss: -0.20590326 time 1.4504485130310059\n",
      "generator loss: -0.22823828 time 1.4504737854003906\n",
      "generator loss: -0.44132733 time 1.4504404067993164\n",
      "generator loss: -0.39754122 time 1.4413435459136963\n",
      "generator loss: -0.059381478 time 1.44130539894104\n",
      "generator loss: -0.32205486 time 1.4569571018218994\n",
      "generator loss: -0.39019647 time 1.4569306373596191\n",
      "Epoch generator: 10\n",
      "generator loss: -0.07158186 time 1.441347360610962\n",
      "generator loss: 0.020399518 time 1.441330909729004\n",
      "generator loss: -0.23332319 time 1.4726245403289795\n",
      "generator loss: 0.027249765 time 1.4569711685180664\n",
      "generator loss: -0.15102844 time 1.4504621028900146\n",
      "generator loss: -0.04505554 time 1.4660625457763672\n",
      "generator loss: -0.19453001 time 1.4504382610321045\n",
      "generator loss: 0.03040805 time 1.466050148010254\n",
      "generator loss: -0.16198817 time 1.4504132270812988\n",
      "generator loss: 0.05958894 time 1.5819165706634521\n",
      "Epoch generator: 20\n",
      "generator loss: -0.1506996 time 1.5416018962860107\n",
      "generator loss: -0.1700017 time 1.466097354888916\n",
      "generator loss: -0.15741885 time 1.4503965377807617\n",
      "generator loss: -0.56640685 time 1.450439453125\n",
      "generator loss: -0.15068394 time 1.4660515785217285\n",
      "generator loss: 0.02423257 time 1.4504268169403076\n",
      "generator loss: -0.14966947 time 1.4413115978240967\n",
      "generator loss: -0.076719925 time 1.4504139423370361\n",
      "generator loss: -0.05301745 time 1.4504084587097168\n",
      "generator loss: -0.030857757 time 1.4504261016845703\n",
      "Training D ...\n",
      "Epoch: 7\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.46351075172424316\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.47002577781677246\n",
      "4754\n",
      "negative sampling for d using g ...\n",
      "time used in generation 0.4700174331665039\n",
      "4754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.name_scope(\"generator_train\"):\n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(G_LEARNING_RATE)\n",
    "    g_train_op = g_optimizer.minimize(g_loss, var_list=generator_variables)\n",
    "    \n",
    "with tf.name_scope(\"discriminator_train\"):\n",
    "    d_optimizer = tf.train.GradientDescentOptimizer(D_LEARNING_RATE)\n",
    "    d_train_op = d_optimizer.minimize(d_loss, var_list=discriminator_variables)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "## Start graph computations and algorithm\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "p_best_val = 0.0\n",
    "ndcg_best_val = 0.0\n",
    "\n",
    "generate_samples_time = 0\n",
    "generate_epoch_time = 0\n",
    "discriminator_record_time = []\n",
    "\n",
    "discriminator_train_time = 0\n",
    "\n",
    "prob_is_candidates = prob_candidates * (1.0 - LAMBDA)\n",
    "\n",
    "for epoch in range(30):\n",
    "    if epoch >= 0:\n",
    "        # G generate negative for D, then train D\n",
    "        print('Training D ...')\n",
    "        print(\"Epoch:\",epoch)\n",
    "        for d_epoch in range(100):\n",
    "\n",
    "            if d_epoch % 30 == 0:\n",
    "                generate_samples_time = time.time()\n",
    "                selected_samples_X, selected_samples_Y = generate_for_d(sess, DIS_TRAIN_FILE)\n",
    "                print(\"time used in generation\",time.time()-generate_samples_time)\n",
    "                train_size = len(selected_samples_X)\n",
    "                #train_size = ut.file_len(DIS_TRAIN_FILE)\n",
    "                print(train_size)\n",
    "            \n",
    "            discriminator_train_time = time.time()\n",
    "            index = 0\n",
    "            while True:\n",
    "                if index >= train_size:\n",
    "                    break\n",
    "                if index + BATCH_SIZE < train_size:\n",
    "                    #input_pos = selected_samples_pos[index:index+BATCH_SIZE]\n",
    "                    #input_neg = selected_samples_neg[index:index+BATCH_SIZE]\n",
    "                    pred_data = selected_samples_X[index:index+BATCH_SIZE]\n",
    "                    pred_data_label = selected_samples_Y[index:index+BATCH_SIZE]\n",
    "                    #input_pos,input_neg = ut.get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n",
    "                else:\n",
    "                    #input_pos = selected_samples_pos[index:index+train_size - index ] \n",
    "                    #input_neg = selected_samples_neg[index:index+train_size - index ]\n",
    "                    pred_data = selected_samples_X[index:train_size]\n",
    "                    pred_data_label = selected_samples_Y[index:train_size]\n",
    "                    #input_pos,input_neg = ut.get_batch_data(DIS_TRAIN_FILE, index, train_size - index + 1)\n",
    "                index += BATCH_SIZE\n",
    "                \"\"\"\n",
    "                #print(index)\n",
    "            \n",
    "                pred_data = []\n",
    "                pred_data.extend(input_pos)\n",
    "                pred_data.extend(input_neg)\n",
    "                pred_data = np.asarray(pred_data)\n",
    "                \n",
    "                #print(pred_data.shape)\n",
    "                \n",
    "                pred_data_label = [1.0] * len(input_pos)\n",
    "                pred_data_label.extend([0.0] * len(input_neg))\n",
    "                pred_data_label = np.asarray(pred_data_label)\n",
    "                \"\"\"\n",
    "                _ = sess.run(d_train_op,\n",
    "                             feed_dict={X: pred_data,\n",
    "                                        Y: pred_data_label})\n",
    "                \n",
    "            discriminator_record_time += [time.time()-discriminator_train_time]\n",
    "            \n",
    "    # Train G\n",
    "    print('Training G ...')\n",
    "    print(\"AVG discriminator train time\", sum(discriminator_record_time)/len(discriminator_record_time))\n",
    "    for g_epoch in range(30):\n",
    "        \n",
    "        if g_epoch%10==0:\n",
    "            print(\"Epoch generator:\",g_epoch)\n",
    "            \n",
    "        generate_epoch_time = time.time()\n",
    "        \n",
    "        for query in query_pos_train.keys():\n",
    "            pos_list = query_pos_train[query]\n",
    "            pos_set = set(pos_list)\n",
    "            all_list = query_index_url[query]\n",
    "\n",
    "            #all_list_feature = [query_url_feature[query][url] for url in all_list]\n",
    "            all_list_feature = query_all_features[query]\n",
    "            #all_list_score = sess.run(prob_candidates, {X: all_list_feature})\n",
    "\n",
    "            # softmax for all\n",
    "            #exp_rating = np.exp(all_list_score - np.max(all_list_score))\n",
    "            #prob = exp_rating / np.sum(exp_rating)\n",
    "            \n",
    "            prob_IS, prob = sess.run([prob_is_candidates,prob_candidates], {X: all_list_feature})\n",
    "            #prob_IS = prob * (1.0 - LAMBDA)\n",
    "\n",
    "            for i in range(len(all_list)):\n",
    "                if all_list[i] in pos_set:\n",
    "                    prob_IS[i] += (LAMBDA / (1.0 * len(pos_list)))\n",
    "\n",
    "            choose_index = np.random.choice(np.arange(len(all_list)), [5 * len(pos_list)], p=prob_IS)\n",
    "            choose_list = np.array(all_list)[choose_index]\n",
    "            choose_feature = [query_url_feature[query][url] for url in choose_list]\n",
    "            choose_IS = np.array(prob)[choose_index] / np.array(prob_IS)[choose_index]\n",
    "\n",
    "            choose_index = np.asarray(choose_index)\n",
    "            choose_feature = np.asarray(choose_feature)\n",
    "            choose_IS = np.asarray(choose_IS)\n",
    "\n",
    "            _,loss = sess.run([g_train_op,g_loss],\n",
    "                         feed_dict={X: all_list_feature,\n",
    "                                    sample_index: choose_index,\n",
    "                                    X_for_reward: choose_feature,\n",
    "                                    important_sampling: choose_IS})\n",
    "\n",
    "        print(\"generator loss:\",loss,\"time\",time.time()-generate_epoch_time)\n",
    "        p_5 = precision_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "        ndcg_5 = ndcg_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "        \n",
    "        if p_5 > p_best_val:\n",
    "            p_best_val = p_5\n",
    "            ndcg_best_val = ndcg_5\n",
    "            print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n",
    "        elif p_5 == p_best_val:\n",
    "            if ndcg_5 > ndcg_best_val:\n",
    "                ndcg_best_val = ndcg_5\n",
    "                print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
