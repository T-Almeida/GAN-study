{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Generative Advarsarial Network\n",
    "\n",
    "Here is presented the implementation of the following parper https://arxiv.org/abs/1705.10513 with help of their source code https://github.com/geek-ai/irgan\n",
    "\n",
    "Implementation is done in tensorflow using layers API\n",
    "\n",
    "Created by: Tiago Almeida 13/02/2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### imports\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import utils as ut # auxiliar file to help in data visualization\n",
    "from os.path import join\n",
    "\n",
    "### not mine, authors\n",
    "from eval_irgan.precision import precision_at_k\n",
    "from eval_irgan.ndcg import ndcg_at_k\n",
    "\n",
    "import random\n",
    "\n",
    "#tensorflow version when notebook was created - 1.4.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRGAN parameters\n",
    "\n",
    "From the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector size of features of the query-document pair\n",
    "FEATURE_SIZE = 46\n",
    "HIDDEN_SIZE = 46\n",
    "BATCH_SIZE = 8\n",
    "WEIGHT_DECAY = 0.01\n",
    "D_LEARNING_RATE = 0.001\n",
    "G_LEARNING_RATE = 0.001\n",
    "TEMPERATURE = 0.2\n",
    "LAMBDA = 0.5\n",
    "\n",
    "work_directory = join(\"data\",\"MQ2008-semi\")\n",
    "\n",
    "DIS_TRAIN_FILE = join(work_directory,\"run-train-gan.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset MQ2008-semi\n",
    "\n",
    "I'm using the same dataset as the authors https://drive.google.com/drive/folders/0B-dulzPp3MmCM01kYlhhNGQ0djA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data = join(work_directory,\"train.txt\")\n",
    "test_data = join(work_directory,\"test.txt\")\n",
    "large_norm = join(work_directory,\"Large_norm.txt\")\n",
    "\n",
    "#get features of all query-document par from test and train set\n",
    "query_url_feature, query_url_index, query_index_url = ut.load_all_query_url_feature(large_norm, FEATURE_SIZE)\n",
    "\n",
    "#query-document pairs with positive relevance from train set\n",
    "query_pos_train = ut.get_query_pos(train_data)\n",
    "\n",
    "#query-document pairs with positive relevance from test set\n",
    "query_pos_test = ut.get_query_pos(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, FEATURE_SIZE], name=\"input_generator\")\n",
    "\n",
    "def generator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1]) / TEMPERATURE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"pred_data_label\")\n",
    "\n",
    "def discriminator(x,name):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\",reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        fc1 = tf.layers.dense(x, HIDDEN_SIZE, activation=tf.nn.tanh, name = name+'_hidden1')\n",
    "        \n",
    "        #output \n",
    "        return tf.reshape(tf.layers.dense(fc1, 1, activation=None, name = name+'_output'), [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_reward = tf.placeholder(tf.float32, shape=[None], name='reward')\n",
    "sample_index = tf.placeholder(tf.int32, shape=[None], name='sample_index')\n",
    "important_sampling = tf.placeholder(tf.float32, shape=[None], name='important_sampling')\n",
    "\n",
    "gen_scores = generator(X,\"g\")\n",
    "dis_pred = discriminator(X,\"d\")\n",
    "\n",
    "generator_variables = [var for var in tf.trainable_variables() if 'g_' in var.name]\n",
    "discriminator_variables = [var for var in tf.trainable_variables() if 'd_' in var.name]\n",
    "\n",
    "with tf.name_scope(\"generator_loss\"):\n",
    "    gen_scores_prob = tf.nn.softmax(tf.reshape(gen_scores, [1, -1]))\n",
    "    \n",
    "    gan_prob = tf.gather(tf.reshape(gen_scores_prob, [-1]), sample_index)\n",
    "\n",
    "    g_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in generator_variables])\n",
    "    \n",
    "    g_loss = -tf.reduce_mean(tf.log(gan_prob) * g_reward * important_sampling) + WEIGHT_DECAY * g_weight_decay\n",
    "        \n",
    "with tf.name_scope('discriminator_loss'):\n",
    "    d_weight_decay = tf.reduce_sum([tf.nn.l2_loss(var) for var in discriminator_variables])\n",
    "    \n",
    "    d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_pred, labels=Y)) + WEIGHT_DECAY * d_weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate (select) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_for_d(sess, filename):\n",
    "    input_pos = []\n",
    "    input_neg = []\n",
    "    \n",
    "    print('negative sampling for d using g ...')\n",
    "    for query in query_pos_train:\n",
    "        \n",
    "        \n",
    "        pos_list = query_pos_train[query] #positive documents\n",
    "        all_list = query_index_url[query] #all documents\n",
    "        candidate_list = all_list\n",
    "\n",
    "        #get features for possible selected documents\n",
    "        candidate_list_feature = [query_url_feature[query][url] for url in candidate_list]\n",
    "        candidate_list_feature = np.asarray(candidate_list_feature)\n",
    "\n",
    "        # softmax for candidate\n",
    "        prob_candidates = tf.nn.softmax(gen_scores - tf.reduce_max(gen_scores))\n",
    "        prob = sess.run(prob_candidates, feed_dict={X: candidate_list_feature})\n",
    "\n",
    "        #exp_rating = np.exp(candidate_list_score - np.max(candidate_list_score))\n",
    "        #prob = exp_rating / np.sum(exp_rating)\n",
    "        \n",
    "        # from all candidate choose same numbers of positive ones\n",
    "        neg_list = np.random.choice(candidate_list, size=[len(pos_list)], p=prob)\n",
    "        \n",
    "        for i in range(len(pos_list)):\n",
    "            input_pos.append(query_url_feature[query][pos_list[i]])\n",
    "            input_neg.append(query_url_feature[query][neg_list[i]])\n",
    "    \n",
    "\n",
    "    #save in disk\n",
    "    return input_pos, input_neg\n",
    "    \n",
    "    return data\n",
    "    print(\"num elements\",len(data))\n",
    "    with open(filename, 'w') as fout:\n",
    "        for (q, pos, neg) in data:\n",
    "            fout.write(','.join([str(f) for f in query_url_feature[q][pos]])\n",
    "                       + '\\t'\n",
    "                       + ','.join([str(f) for f in query_url_feature[q][neg]]) + '\\n')\n",
    "            fout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training D ...\n",
      "Epoch: 0\n",
      "negative sampling for d using g ...\n",
      "time used in generation 48.096176862716675\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'selected_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9e59087640c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mselected_samples_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected_samples_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_for_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIS_TRAIN_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time used in generation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgenerate_samples_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'selected_samples' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.name_scope(\"generator_train\"):\n",
    "    g_optimizer = tf.train.GradientDescentOptimizer(G_LEARNING_RATE)\n",
    "    g_train_op = g_optimizer.minimize(g_loss, var_list=generator_variables)\n",
    "    \n",
    "with tf.name_scope(\"discriminator_train\"):\n",
    "    d_optimizer = tf.train.GradientDescentOptimizer(D_LEARNING_RATE)\n",
    "    d_train_op = d_optimizer.minimize(d_loss, var_list=discriminator_variables)\n",
    "\n",
    "    d_reward = (tf.sigmoid(dis_pred) - 0.5) * 2\n",
    "\n",
    "\n",
    "## Start graph computations and algorithm\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "p_best_val = 0.0\n",
    "ndcg_best_val = 0.0\n",
    "\n",
    "generate_samples_time = 0\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    if epoch >= 0:\n",
    "        # G generate negative for D, then train D\n",
    "        print('Training D ...')\n",
    "        print(\"Epoch:\",epoch)\n",
    "        for d_epoch in range(100):\n",
    "\n",
    "            if d_epoch % 30 == 0:\n",
    "                generate_samples_time = time.time()\n",
    "                selected_samples_pos, selected_samples_neg = generate_for_d(sess, DIS_TRAIN_FILE)\n",
    "                print(\"time used in generation\",time.time()-generate_samples_time)\n",
    "                train_size = len(selected_samples)\n",
    "\n",
    "            index = 1\n",
    "            while True:\n",
    "                if index > train_size:\n",
    "                    break\n",
    "                if index + BATCH_SIZE <= train_size + 1:\n",
    "                    input_pos = selected_samples_pos[index:index+BATCH_SIZE]\n",
    "                    input_neg = selected_samples_neg[index:index+BATCH_SIZE]#ut.get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n",
    "                else:\n",
    "                    input_pos = selected_samples_pos[index:index+train_size - index + 1] #ut.get_batch_data(DIS_TRAIN_FILE, index, train_size - index + 1)\n",
    "                    input_neg = selected_samples_neg[index:index+train_size - index + 1]\n",
    "                index += BATCH_SIZE\n",
    "\n",
    "                pred_data = []\n",
    "                pred_data.extend(input_pos)\n",
    "                pred_data.extend(input_neg)\n",
    "                pred_data = np.asarray(pred_data)\n",
    "\n",
    "                pred_data_label = [1.0] * len(input_pos)\n",
    "                pred_data_label.extend([0.0] * len(input_neg))\n",
    "                pred_data_label = np.asarray(pred_data_label)\n",
    "\n",
    "                _ = sess.run(d_train_op,\n",
    "                             feed_dict={X: pred_data,\n",
    "                                        Y: pred_data_label})\n",
    "    # Train G\n",
    "    print('Training G ...')\n",
    "    for g_epoch in range(30):\n",
    "        if g_epoch%10==0:\n",
    "            print(\"Epoch generator:\",g_epoch)\n",
    "        for query in query_pos_train.keys():\n",
    "            pos_list = query_pos_train[query]\n",
    "            pos_set = set(pos_list)\n",
    "            all_list = query_index_url[query]\n",
    "\n",
    "            all_list_feature = [query_url_feature[query][url] for url in all_list]\n",
    "            all_list_feature = np.asarray(all_list_feature)\n",
    "            all_list_score = sess.run(gen_scores, {X: all_list_feature})\n",
    "\n",
    "            # softmax for all\n",
    "            exp_rating = np.exp(all_list_score - np.max(all_list_score))\n",
    "            prob = exp_rating / np.sum(exp_rating)\n",
    "\n",
    "            prob_IS = prob * (1.0 - LAMBDA)\n",
    "\n",
    "            for i in range(len(all_list)):\n",
    "                if all_list[i] in pos_set:\n",
    "                    prob_IS[i] += (LAMBDA / (1.0 * len(pos_list)))\n",
    "\n",
    "            choose_index = np.random.choice(np.arange(len(all_list)), [5 * len(pos_list)], p=prob_IS)\n",
    "            choose_list = np.array(all_list)[choose_index]\n",
    "            choose_feature = [query_url_feature[query][url] for url in choose_list]\n",
    "            choose_IS = np.array(prob)[choose_index] / np.array(prob_IS)[choose_index]\n",
    "\n",
    "            choose_index = np.asarray(choose_index)\n",
    "            choose_feature = np.asarray(choose_feature)\n",
    "            choose_IS = np.asarray(choose_IS)\n",
    "\n",
    "            choose_reward = sess.run(d_reward, feed_dict={X: choose_feature})\n",
    "\n",
    "            _ = sess.run(g_train_op,\n",
    "                         feed_dict={X: all_list_feature,\n",
    "                                    sample_index: choose_index,\n",
    "                                    g_reward: choose_reward,\n",
    "                                    important_sampling: choose_IS})\n",
    "\n",
    "\n",
    "        p_5 = precision_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "        ndcg_5 = ndcg_at_k(sess,X, gen_scores, query_pos_test, query_pos_train, query_url_feature, k=5)\n",
    "\n",
    "        if p_5 > p_best_val:\n",
    "            p_best_val = p_5\n",
    "            ndcg_best_val = ndcg_5\n",
    "            print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n",
    "        elif p_5 == p_best_val:\n",
    "            if ndcg_5 > ndcg_best_val:\n",
    "                ndcg_best_val = ndcg_5\n",
    "                print(\"Best:\", \"gen p@5 \", p_5, \"gen ndcg@5 \", ndcg_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
